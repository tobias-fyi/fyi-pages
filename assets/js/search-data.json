{
  
    
        "post0": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7fe0ab662790&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from pathlib import Path p = Path(&#39;../_notebooks&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#21) [Path(&#39;../_notebooks/gpt2_simple_mask.jpg&#39;),Path(&#39;../_notebooks/bert_mac_small.jpg&#39;),Path(&#39;../_notebooks/causal_with_prefix.jpg&#39;),Path(&#39;../_notebooks/.DS_Store&#39;),Path(&#39;../_notebooks/2020-03-07-How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.ipynb&#39;),Path(&#39;../_notebooks/2020-09-01-fastcore.ipynb&#39;),Path(&#39;../_notebooks/2020-03-07-Syntax-Highlighting.ipynb&#39;),Path(&#39;../_notebooks/2020-03-06-bart.ipynb&#39;),Path(&#39;../_notebooks/README.md&#39;),Path(&#39;../_notebooks/2020-05-01-TrainDonkeyCar.ipynb&#39;)...] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.foundation module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [2,0,18,6,15,17,14,8,12,1...] . Index into a list: . p[2,4,6] . (#3) [18,15,14] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://tobias-fyi.github.io/fyi-pages/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Automated Image Background Removal with Python",
            "content": ". Notes . Please do not request access to this tutorial. You can make a copy of this tutorial by &quot;File -&gt; Open in playground mode&quot; and play with it to your heart&#39;s content. | . Resources and related content . Trash Panda | Two Shots to Green Screen: Collage with Deep Learning | . . . Introduction . TL;DR . The goal of this tutorial is to describe one method of automating the process of cutting out objects (things, people, pets, etc.) from images and combining them to make a collage of sorts. . First, I go through creating binary masks for one or more objects in an image by using a class of computer vision algorithms called image segmentation. Binary mask(s) in hand(s), I go through one method (technically two, actually) of using said binary mask(s) to extract or remove part(s) of an image. Next, I do some basic image transformations (rotate, crop, and scale) on the resulting cutout. Finally, I paste the cutout on top of another image to make a collage. . Rather than drawing binary masks by hand or using proprietary software like Photoshop to manipulate and transform images, I&#39;ll show you how to automate the process using completely free, open-source tools. Namely, we&#39;ll be using Python along with a few open-source libraries: . NumPy | OpenCV - opencv-python | PyTorch - Detectron2 | Pillow | . . The Problem . Selecting and separating parts of an image can be a tedious, time-consuming process. Anyone who&#39;s done a fair amount of tinkering with image manipulation using a program like Photoshop knows the struggle. . Although modern tools make the process easier, wouldn&#39;t it be nice if there was a way to automate the process? . Creating &quot;Paws&quot; . As an example, say I&#39;d like to cut out my cat Hobbes from a photo in order to &quot;Photoshop&quot; him into a different image. Here&#39;s the photo of Hobbes I&#39;ll be using. . . I think his position is perfect for creating &quot;Hawbbes&quot; (Jaws + Hobbes)...meh I&#39;ll call it &quot;Paws&quot;. By cutting him out and rotating him a bit, he could be pasted onto an underwater shot of someone swimming and he could live his dream of being a fierce sharkitty. . Here&#39;s an image I found on Unsplash that would work as the background onto which Hobbes, once cut out of the image above, can be pasted. . . Basically, in order to cut Hobbes out of the photo above, I&#39;ll have to make all the pixels in the image transparent except for the ones representing Hobbes. Then I&#39;ll crop, rotate, scale, and superimpose the resulting image on top of the underwater shot such that Hobbes roughly takes up the bottom half. . . . Solution . Image Masking . To accomplish this manually, I could spend anywhere from a few minutes to a few hours outlining Hobbes in the original image to create a mask — masking the image. The time investment depends on how easily-separable the subject is from the rest of the image, how accurate I want the cut to be, and what tools are available to me. . Regarding that last point, the magicians at Adobe have done some rather impressive black magic with Photoshop, giving users very quick and very effective methods for selecting parts of an image. However, the goal of this post is to accomplish this programmatically, without the use of any closed-source software. . A mask is basically a method of distinguishing/selecting/separating pixels. If you&#39;ve never heard the term used this way before, one way to think about it is with masking tape and paint. Typically, one would put masking tape—i.e. create a &quot;mask&quot;—around areas on a wall that should not be painted. This is essentially what a mask is doing in any photo manipulation software: indicating what areas of an image to affect or transform (or which areas not to do so). . Here&#39;s the image of Hobbes with the image segmentation-generated masks overlayed on top of it (which we&#39;ll be creating later) showing, obviously, where Hobbes is in the image. It doesn&#39;t really matter that the model thinks he&#39;s a dog — we won&#39;t be using the predicted class, only the mask. And the mask is still good enough for our purposes. . . A binary mask is a method of masking which uses a two-tone color scheme, to indicate the areas of an image to be affected and not affected. By overlaying a binary mask on top of the original image, the boundaries between the two colors can be used to affect the different areas of the image differently, whether that is making pixels transparent (removing them) or applying some sort of effect or transformation. . The white area in the image below shows the same coordinates as the orange one above, converted into a binary mask. While I&#39;ve only spent any significant time with Photoshop, I&#39;d imagine any decent image manipulation software can work with binary masks similarly to how we&#39;ll be working with them. . . Computer vision . In order to generate binary masks based on the content of the image, the algorithm must be somewhat intelligent. That is, it must be able to process the image in such a way that it can recognize where the foreground is and draw a polygon around it with some degree of accuracy. . Luckily, there are a number of deep learning models that will do just that. The field is called Computer Vision, and the class of algorithm used in this article is known as image segmentation. . Don&#39;t worry if you don&#39;t have any experience with this type of thing, or even if you don&#39;t necessarily want to get experience with it. Modern machine learning tooling makes it incredibly quick and easy to get a model up and predicting with pre-trained weights. Though if you want to understand what&#39;s going on, it will likely help to know a bit of Python programming. . One caveat: the pre-trained models will usually work well with classes of objects that were in their training data. The model weights used in this post were trained on the COCO dataset, which contains 80 object classes. Depending on what the object in the foreground is that you are trying to extract, you may or may not need to extend the model with a custom dataset and training session. That is a topic for another post. . Detectron2 . The deep learning framework used here is PyTorch, developed by Facebook AI Research (FAIR). More specifically, we&#39;ll use a computer vision framework, also developed by FAIR, called Detectron2. . Although the primary framework used in this article is Detectron2, this process should be translatable to other image segmentation models as well. In fact, I&#39;ll be adding an addendum to this post in which I&#39;ll go over using Matterport&#39;s TensorFlow-based implementation of Mask R-CNN to accomplish the exact same thing. . Heck, while I&#39;m at it, I might as well do it with fastai as well. . End Result . I know you&#39;ve been dying to see the end result of the whole process. . Without any further ado, I present to you, Paws! . . [[Caption :: All of that was done with code. Pretty neat, eh?]] With that, let&#39;s get into how this masterpiece was created. . . . Setup . Install Detectron2 and other dependencies . As mentioned in the introduction, the framework we&#39;ll be using for image segmentation is called Detectron2. The following cells install and set up Detectron2 in a Google Colab environment (pulled from the official Detectron2 getting started notebook). If you don&#39;t want to use Colab for whatever reason, either play around with installation and setup or refer to the installation instructions. . The other top-level dependencies needed for this tutorial: . NumPy | opencv-python | Pillow | . The nice thing about Colab is all of these come pre-installed. Oh yeah, you also get free access to a GPU. Thanks, Googs! . Again, simply click the &quot;Open in Colab&quot; badge at the top of this page, then hit File &gt; Save a copy in Drive, which does exactly what it says: saves a copy of the notebook to your Google Drive. In addition, you can open an ephemeral copy of the notebook without saving it first by hitting File &gt; Open in playground mode. . Once you have everything installed, we can start with some imports and configuration. . #collapse-hide # === Some imports and setup === # # Setup Detectron2 logger import detectron2 from detectron2.utils.logger import setup_logger setup_logger() # Common libraries import numpy as np import os, json, cv2, random # Only needed when running in Colab from google.colab.patches import cv2_imshow # Detectron2 utilities from detectron2 import model_zoo from detectron2.engine import DefaultPredictor from detectron2.config import get_cfg from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog, DatasetCatalog . . . . Running a pre-trained Detectron2 model . Most, if not all, open-source deep learning frameworks have a set of pre-trained weights available to use. The creators of the frameworks will conduct a series of training sessions on the most commonly-used datasets in order to benchmark the performance of their algorithms. Luckily for everyone else, they typically provide the results of this training in the form of weights, which can be loaded into the model and be used for inference immediately. . For many tasks, including recognizing and outlining an image of a cat, pre-trained weights will work fine. The model weights used in this post were trained on the popular COCO dataset, which contains 80 object classes, including cats. If, for example, we wanted to do the same thing with whales or one specific person, we&#39;d have to do some custom training. . I will be publishing a companion blog post to this one about training Detectron2 on a custom dataset. Once that is published, I&#39;ll link to it here. If there&#39;s no link yet, I haven&#39;t published it yet. . If you&#39;re curious about custom training now, the Detectron2 &quot;Getting Started&quot; Colab notebook also goes through one way of doing so. . Image loading and image arrays . The first thing we need in order to use the model is an image on which it can be used. . We first download the image to the local filesystem using wget, then load it into memory using cv2 (opencv-python). . # === Download and load image === # !wget https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/01_hobbes.jpg im = cv2.imread(&quot;./01_hobbes.jpg&quot;) . --2020-07-20 19:35:16-- https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/01_hobbes.jpg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 344670 (337K) [image/jpeg] Saving to: ‘01_hobbes.jpg’ 01_hobbes.jpg 100%[===================&gt;] 336.59K --.-KB/s in 0.05s 2020-07-20 19:35:16 (7.01 MB/s) - ‘01_hobbes.jpg’ saved [344670/344670] . If you think about what a digital image actually is, it makes sense to represent it as a matrix — each row corresponds to a row of pixels, and each column a column of pixels in the image. Technically, images would be considered a 3-dimensional array, because they have width, height, and depth (number of channels). . Depending on if the image has three channels (typically RGB: red, green, blue) or four (typically RGBA: same plus an alpha channel), the values at each row-column index (or cell, like in a spreadsheet, in case that helps you visualize it) indicate the intensities of each of the color channels (and transparency, in the case of 4-channel images) for each pixel. . Thus, after the image is loaded, it really is just an array of numbers and can be utilized and manipulated just like any other array. For example, in order to rotate the image, a linear transformation can be applied to the image matrix to &quot;rotate&quot; the pixel values within the matrix. . Here is an example of a single row in the array representing the image of Hobbes is shown. . #collapse-hide # === Look at the image, in array form === # print(&quot;Image dimensions:&quot;, im.shape) print(&quot; nImage array - first row of 3-value sub-arrays:&quot;) im[0] . . Image dimensions: (800, 600, 3) Image array - first row of 3-value sub-arrays: . array([[172, 192, 209], [188, 208, 225], [119, 137, 154], ..., [137, 151, 149], [139, 153, 151], [142, 156, 154]], dtype=uint8) . #collapse-hide # === Look at the image, rendered === # cv2_imshow(im) . . . Inference with Detectron2 . After the image is loaded, we&#39;re ready to use Detectron2 to run inference on the image and find the mask of Hobbes. Running inference means generating predictions from the model. In the case of image segmentation, the model is making a prediction for each pixel, providing its best guess at what class of object each one belongs to, if any. . We create a Detectron2 config and instantiate a DefaultPredictor, which is then used to run inference. . Just a heads-up: the first time this runs, it will automatically attempt to start downloading the pre-trained weights — a ~180mb pickle file. That&#39;s a lot of pickles... . In addition to downloading and configuring the weights, the threshold is set for the minimum predicted probability. In other words, the model will only output a prediction if it is certain enough — the probability assigned to the prediction is above the threshold. . # === Set up config and run inference === # # Add project-specific config (e.g., TensorMask) here if you&#39;re # not running a model in Detectron2&#39;s core library cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)) cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold for this model # Find a model from detectron2&#39;s model zoo. You can use the https://dl.fbaipublicfiles... url as well cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;) predictor = DefaultPredictor(cfg) outputs = predictor(im) . model_final_f10217.pkl: 178MB [00:05, 29.7MB/s] . By default, the output of the model contains a number of results, including the predicted classes, coordinates for the bounding boxes (object detection), and mask arrays (image segmentation), along with various others, such as pose estimation (for people). More information on the types of predictions made by Detectron2 can be found in the documentation. . We are really only interested in the one mask outlining Mr. Hobbes here, though will also need to extract the IDs for the predicted classes in order to select the correct mask. If the image only has one type of object, then this part isn&#39;t really necessary. But when there are many different classes in a single image, it&#39;s important to be certain which object we are extracting. . First, let&#39;s take a look at how the model did. . # `Visualizer` draws the predictions on the image v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8) out = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) cv2_imshow(out.get_image()[:, :, ::-1]) . . Extracting the mask . The &quot;dog&quot; is the first id in the .pred_classes list. Therefore, the mask that we want is the first one in the .pred_masks tensor (array). . Those colored areas are the &quot;masks&quot;, which can be extracted from the output of the model and used to manipulate the image in neat ways. First, we&#39;ll need to get the array holding the mask. . In this case, as can be seen below, each mask is a 2-dimensional array of Boolean values, each one representing a pixel. If a pixel has a &quot;True&quot; value, that means it is inside the mask, and vice-versa. . # Look at the outputs - classes and masks print(outputs[&quot;instances&quot;].pred_classes) print(outputs[&quot;instances&quot;].pred_masks) . tensor([16, 57, 57], device=&#39;cuda:0&#39;) tensor([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]], device=&#39;cuda:0&#39;) . # === Find out what the class IDs correspond to === # List of all classes in training dataset (COCO) # predictor.metadata.as_dict()[&quot;thing_classes&quot;] # The class we are interested in predictor.metadata.as_dict()[&quot;thing_classes&quot;][16] . &#39;dog&#39; . # === Extract (only) the mask and box of Hobbes === # # Find the index of the class we are interested in # First, convert to numpy array to allow direct indexing class_ids = np.array(outputs[&quot;instances&quot;].pred_classes.cpu()) class_index = np.where(class_ids == 16) # Find index where class ID is 16 # Use that index to index the array of masks and boxes mask_tensor = outputs[&quot;instances&quot;].pred_masks[class_index] print(mask_tensor.shape) mask_tensor . torch.Size([1, 800, 600]) . tensor([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]], device=&#39;cuda:0&#39;) . # Convert tensor to numpy array hobbes_mask = mask_tensor.cpu() print(&quot;Before:&quot;, type(hobbes_mask)) print(hobbes_mask.shape) hobbes_mask = np.array(hobbes_mask[0]) print(&quot;After:&quot;, type(hobbes_mask)) print(hobbes_mask.shape) . Before: &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([1, 800, 600]) After: &lt;class &#39;numpy.ndarray&#39;&gt; (800, 600) . hobbes_mask . array([[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]) . . Converting to a binary mask . Now that we&#39;ve run inference on the image and retrieved the mask array, it&#39;s time to turn that array into a binary mask. While I won&#39;t be using this particular binary mask directly, it can be downloaded as a png and/or edited and used to various ends. . # Create blank black background # The &quot;True&quot; pixels will be converted to white and copied onto the black background background = np.zeros(hobbes_mask.shape) background.shape . (800, 600) . # Add white pixels where mask values are True bin_mask = np.where(hobbes_mask, 255, background).astype(np.uint8) print(bin_mask.shape) bin_mask . (800, 600) . array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) . # Show the resulting binary mask cv2_imshow(bin_mask) . . Using the binary mask to cut out Hobbes . In order to use numpy operations between the mask and image, the dimensions of the mask must match the image. The image array has three values for each pixel, indicating the values of red, green, and blue (RGB) that the pixel should render. Therefore, the mask must also have three values for each pixel. To do this, I used a NumPy method called np.stack to basically &quot;stack&quot; three of the masks on top of one another. . Once the dimensions match, another NumPy method, np.where, can be used to copy or extract only the pixels contained within the area of the mask. I created a blank background onto which those pixels are copied. . # === Add a fourth channel to the original image array === # # Split into RGB (technically BGR in OpenCV) channels b, g, r = cv2.split(im.astype(&quot;uint8&quot;)) # Create alpha channel array of ones # Then multiply by 255 to get the max transparency value a = np.ones(hobbes_mask.shape, dtype=&quot;uint8&quot;) * 255 print(b.shape, g.shape, r.shape, a.shape) . (800, 600) (800, 600) (800, 600) (800, 600) . # === Alpha value of 255 means fully opaque === # # We want the image to be fully opaque at this point a . array([[255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], ..., [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255]], dtype=uint8) . # === Merge image channels back together === # # Rejoin with alpha channel that&#39;s always 1, or non-transparent rgba = [b, g, r, a] # Both of the lines below accomplish the same thing im_4ch = cv2.merge(rgba, 4) # im_4ch = np.stack([b, g, r, a], axis=2) print(im_4ch.shape) cv2_imshow(im_4ch) . (800, 600, 4) . # === Extract pixels using mask === # # Create 4-channel blank background bg = np.zeros(im_4ch.shape) print(&quot;BG shape:&quot;, bg.shape) # Create 4-channel mask mask = np.stack([hobbes_mask, hobbes_mask, hobbes_mask, hobbes_mask], axis=2) print(&quot;Mask shape:&quot;, mask.shape) # Copy color pixels from the original color image where mask is set foreground = np.where(mask, im_4ch, bg).astype(np.uint8) # Check out the result cv2_imshow(foreground) . BG shape: (800, 600, 4) Mask shape: (800, 600, 4) . . The Roundabout Method . This is that &quot;second&quot; method I talked about in the introduction. . This is how I added a fourth channel to the image after the fact, once the colored pixels had been copied onto a black background. While this method works, I&#39;m sure you can think of one primary issue with it. . It took me too long to realize this, but by using a black background and the method below, which converts all black pixels to transparent, any pixels brought over from the original image that also happened to be black were converted to transparent. . That&#39;s why I decided to refactor into the method above. . However, I felt like I should leave it in anyways, as it still has some potentially useful code in it. For example, in the case when the image cannot be converted to four channels beforehand. . # Create a blank black 3-channel background bg = np.zeros(im.shape) bg.shape . (800, 600, 3) . # Create 3-channel mask mask = np.stack([hobbes_mask, hobbes_mask, hobbes_mask], axis=2) . # Copy color pixels from the original color image where mask is set foreground = np.where(mask, im, bg).astype(np.uint8) . # Convert to 4-channel image # i.e. add the alpha channel and convert black pixels to alpha tmp = cv2.cvtColor(foreground.astype(&quot;uint8&quot;), cv2.COLOR_BGR2GRAY) _, alpha = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY) b, g, r = cv2.split(foreground.astype(&quot;uint8&quot;)) rgba = [b, g, r, alpha] dst2 = cv2.merge(rgba, 4) # Look at the result, if needed # cv2_imshow(dst2) . . . Image manipulation with Python . Now, this image can be saved (as a PNG to preserve the alpha channel/transparency) and simply overlayed onto another image. Or, even better, the image can be used directly (as it is now, in the form of an array), scaled, rotated, moved, then pasted overtop of the other image. . At first I was going to use Photoshop to overlay Hobbes and make him look like a super dangerous sharkitty. But then I remembered the goal of this post, and decided to do it programmatically with Python. . The primary library I&#39;ll be using to manipulate images is Pillow. . # === Import Pillow === # from PIL import Image # Use plt to display images import matplotlib.pyplot as plt %matplotlib inline . # === Download the background image === # !wget https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/05_jaws_kinda.jpg -q -O 05_jaws_kinda.jpg . # === Load background image from file === # jaws_img = Image.open(&quot;05_jaws_kinda.jpg&quot;) # Dimensions of background image (600, 900) will be useful later print(jaws_img.size) plt.imshow(jaws_img) . (600, 900) . &lt;matplotlib.image.AxesImage at 0x7f97f3cd0748&gt; . . Rotation . I decided to rotate the image matrix directly using OpenCV, prior to loading it into Pillow. There is a .rotate method on Pillow&#39;s Image class as well, which could accomplish the same thing. Either way works — I just wanted to learn how to do it with OpenCV. . # === Function to rotate custom amount === # # Found here: https://stackoverflow.com/a/9042907/10589271 # There is another (potentially easier) way to do it with Pillow, using Image.rotate() def rotate_image(image, angle): image_center = tuple(np.array(image.shape[1::-1]) / 2) rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0) result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR) return result . # === Rotate the foreground === # fg_rotated = rotate_image(foreground, 45) cv2_imshow(fg_rotated) . Load into Pillow . Once the image is rotated, it needs to be cropped and scaled appropriately. I decided to use Pillow for these transformations. For some reason that I have not looked into yet, OpenCV images are in BGR (blue, green, red) format instead of the virtually universal RGB format. Thus, in order to load the image from cv2 into Pillow without reversing the blues and reds, the color first must be converted to RGB. . Once converted, it can simply be loaded into Pillow as a PIL.Image object, which contains a suite of useful methods for transformation and more. . # === Load image into PIL.Image === # # Convert color from BGRA to RGBA fg_rotated_fixed = cv2.cvtColor(fg_rotated, cv2.COLOR_BGRA2RGBA) # Load into PIL.Image from array in memory hobbes_rotated = Image.fromarray(fg_rotated_fixed) plt.imshow(hobbes_rotated) . &lt;matplotlib.image.AxesImage at 0x7f706946a470&gt; . Crop . I manually defined the coordinates of the box used to crop the image by eyeballing it (shout out to Matplotlib for including tick marks on rendered images). A more automated method for doing this would be to extract the bounding box coordinates from the model output, and use that to crop the image before doing any other transformations. I will add this to a later iteration of this tutorial (if you&#39;re reading this, it likely hasn&#39;t been implemented yet). . # === Crop === # box = (0, 80, 480, 500) crop = hobbes_rotated.crop(box2) print(crop.size) plt.imshow(crop) . (480, 420) . &lt;matplotlib.image.AxesImage at 0x7f70693ac208&gt; . Resize . Next, the cropped and rotated image must be resized in order to fill up the entire width of the background onto which it will be pasted. The reason for this is simply because the dimensions of the &quot;paste&quot; box must exactly match that of the &quot;copy&quot; box. It must be explicit — i.e. it&#39;s not like Photoshop where any of the image falling outside the boundaries of the &quot;canvas&quot; is cropped or simply left out. . # === Scale up to match width === # width = jaws_img.size[0] scale = width / crop.size[0] # Calculate scale to match width height = int(scale * crop.size[1]) # Scale up height accordingly new_size = (width, height) # Resize! resized = crop.resize(new_size) print(resized.size) plt.imshow(resized) . (600, 525) . &lt;matplotlib.image.AxesImage at 0x7f7069ca30b8&gt; . Save! . Finally, with the rotated, cropped, and resized image of Hobbes all ready to go, we can paste it onto the background and save the result! . # === Paste onto background image === # paws = jaws_img.copy() # Paste box dimensions have to exactly match the image being pasted paste_box = (0, paws.size[1] - resized.size[1], paws.size[0], paws.size[1]) paws.paste(resized, paste_box, mask=resized) plt.imshow(paws) . &lt;matplotlib.image.AxesImage at 0x7f706c1ae550&gt; . # === Save the masterpiece to local filesystem === # paws.save(&quot;06_paws.jpg&quot;) . . . . Final Thoughts . What a masterpiece! . Potential improvements . As with just about any process, there are always aspects that could be changed in an effort to improve efficiency or ease-of-use. I will keep adding to this list of potential improvements as I think or hear of them: . Fully automated the crop by using the bounding box from the model output | . Further Work . There are a couple of additional adjacent projects to this that I&#39;d like to work on at some point in the future, both to help others use the method outlined above, and to give me practice with various other aspects of the development lifecycle. . The two projects I have in mind are essentially the same thing, accomplished different ways. First, I&#39;d like to integrate this method into a Python package and put it on PyPI so it can be installed with Pip and easily customized/used from the command line or in other scripts. Second, I want to build a simple web app and API that would allow anyone to upload an image, choose a class of object to extract (or remove), run the model, then download the image with the background (and/or other objects) removed. . As I work on these, I&#39;ll put links here. .",
            "url": "https://tobias-fyi.github.io/fyi-pages/computer%20vision/image%20processing/automation/2020/07/20/remove-bg-python.html",
            "relUrl": "/computer%20vision/image%20processing/automation/2020/07/20/remove-bg-python.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Trash Panda",
            "content": ". Introduction . The Problem . You have an object in your hand that you intend to throw away. When you think about it as you&#39;re walking to the bins, you realize you actually don&#39;t know whether this type of object is recyclable or not. Maybe it is made of multiple different materials, or of an uncommon or unrecognizable material. . You&#39;re in the middle of an important project, and it&#39;s crunch time—no extra time available to spend researching. You end up throwing it in the recycling because it...well, it seems like something that would be recyclable. With the decision made and action taken, you return to your important project, forgetting all about what just transpired. . I&#39;d bet that most who are reading this have had an experience like this. . The priceless time and energy spent researching how to properly dispose of every single item can add up. However, the US is in something of a recycling crisis at the moment, partially due to the low quality of our recyclable waste—it tends to be very intermixed with non-recyclables. . Ever since China&#39;s National Sword legislation in 2017, which vastly reduced the amount of foreign recycling—particularly low-quality—the country would accept, recycling companies in the US have been forced to grapple with this quality issue. The cost of recycling increases when more trash is intermingled with it, as more sorting has to occur prior to processing. Whether it is more expensive machines or higher cost of labor, sorting costs money. . While the domestic recycling infrastructure will (hopefully) grow to meet the increasing demand, the best way to solve a problem is to address the source of the issue, not the symptoms. One key reason for the low quality recycling is simply a lack of easily accessible information. Even with the power of modern search engines at our fingertips, finding relevant recycling information can take a long time, as what exactly constitutes recycling changes depending on area and company. . The simple fact is that most people don&#39;t want to spend the additional time it takes (at least up front) to have good recycling habits. So why not simply remove that additional time from the equation? . . The Solution . The goal was to build an app that helps to foster better recycling habits by reducing the the effort needed to find accurate and relevant information on how to properly dispose of any given item of waste. To make this possible, we needed to reduce the friction so much so that looking up how to dispose of something that a user is holding in their hand is just as quick and easy as debating for a few moments on what bin it goes in. . Put another way, our goal was to reduce the cognitive tax of getting relevant recycling information so much that disposing of every item of waste properly, regardless of what it is, becomes effortless. . Our stakeholder envisioned that the user would simply snap a photo of something they are about to toss. Then, the app&#39;s computer vision (object detection) functionality would recognize the object and automatically pull up the relevant information on how it should be disposed of according to the user&#39;s location and available services. The user would know immediately if the item should be thrown in the trash, recycle, or compost, or if it is recyclable only at an offsite facility. For the latter case, the user would be able to view a list of nearby facilities that accept the item or material. . The result of this vision is a progressive web app (PWA) called Trash Panda, which does just that. You can try out the app on your mobile device now by following the link below. . The Trash Panda app (meant for mobile) . A note on PWAs . For those who aren&#39;t familiar, a PWA is basically a web app that can both be used via the browser and downloaded to the home screen of a mobile device. Google has been moving to fully support PWAs, meaning Trash Panda is available on the Play Store right now. Of course the benefit of a PWA is you don&#39;t actually have to download it at all if you don&#39;t want to. You can use it directly from the browser. . Apple is pretty far behind in their support of PWAs. As a result, the behavior on an iOS device is not ideal. For those on iOS, be sure to use Safari. And when taking a picture of an item, you have to exit out of the video window before pressing the normal shutter button. . You&#39;ll figure it out—we believe in you! . The Team (and My Role On It) . For eight weeks near the beginning of 2020, I worked with a remote interdisciplinary team to bring the vision of Trash Panda to life. . . Trash Panda is by far the most ambitious machine learning endeavor I had yet embarked on. Indeed, it was the largest software project I&#39;d worked on in just about every respect: time, team, ambition, breadth of required knowledge. As such, it provided to me many valuable, foundational experiences that I&#39;ll surely keep with me throughout my entire career. . I seriously lucked out on the team for this project. Every single one of them was hard-working, thoughtful, friendly—a pleasure to work with. The team included myself and three other machine learning engineers, four web developers, and two UX designers (links to all of their respective sites in the Final Thoughts section below). Our stakeholder Trevor Clack, who came up with the idea for the app and pitched it to Labs, also worked on the project as a machine learning engineer. . Trevor&#39;s Trash Panda blog post We all pushed ourselves throughout each and every day of the eight weeks to make Trevor&#39;s vision come to life, learning entirely new technologies, frameworks, skills, and processes along the way. . For example, the web developers taught themselves how to use GraphQL, along with a variety of related/dependent technology. On the machine learning side of things, none of us had significant applied experience with computer vision (CV) going into the project. We&#39;d spent a few days studying and working with it in the Deep Learning unit of our Lambda School curriculum. But that was more to expose us to it, rather than covering the entire process in-depth. We had only the shallowest of surface scratches compared to what was ultimately needed to meet the vision set out for us. . As the machine learning engineers on the team, we were responsible for the entire process of getting an object detection system built, trained, deployed, and integrated with the app. Basically, we were starting from scratch, both in the sense of a greenfield project and of us being inexperienced with CV. . Of course, CV is still machine learning—many steps in the process are similar to any other supervised machine learning project. But working with images comes with its own suite of unique challenges that we had to learn how to overcome. . We split up the work as evenly as possible, given our initially limited knowledge of the details, with some steps being split up between some or all of us, and other steps having a sole owner. . The first step for which I was solely responsible included building a system to automatically remove the background from images (or extract the foreground, depending on how you look at it). Essentially, when tasked with figuring out a way to automate the process of removing the background from images so they could be auto-labeled via a script written by Trevor, I built a secondary pipeline with a pre-trained image segmentation model. More details can be found in the Automated Background Removal section below. . Furthermore, I was responsible for building and deploying the object detection API. I built the API using Flask, containerized it with Docker, and deployed it to AWS Elastic Beanstalk. I go into a little more detail in the Deployment section below, though I will be digging into the code and process of the API much more in a separate blog post. . All members of the machine learning team contributed to the gathering and labeling of the dataset. To this end, each of us ended up gathering and labeling somewhere in the range of 20,000 images, for a total of over 82,000. . . The Data . As is the case with most, if not all, machine learning projects, we spent the vast majority of the time gathering and labeling our dataset. Before we could get into gathering the actual data, however, we needed to define what data we needed to gather and how we were going to gather it. . Classifications . As also seems to be the case with most, if not all, projects in general, we were almost constantly grappling with scope management. In an ideal world, our model would be able to recognize any object that anyone would ever want to throw away. But in reality is this is practically impossible, particularly within the 8 weeks we had to work on Trash Panda. I say &quot;practically&quot; because I&#39;m sure if a company dedicated enough resources to the problem, eventually it could be solved, at least to some degree. . Fortunately, we were granted an API key from Earth911 (shoutout to them for helping out!) to utilize their recycling center search database. At the time we were working with it, the database held information on around 300 items—how they should be recycled based on location, and facilities that accept them if they are not curbside recyclable. They added a number of items when we were already most of the way done with the project, and have likely added more since then. . We had our starting point for the list of items our system should be able to recognize. However, the documentation for the neural network architecture we&#39;d decided to use suggested that to create a robust model, it should be trained with at least 1,000 instances (in this case, images) of each of the classes we wanted it to detect. . Gathering 300,000 images was also quite a bit out of the scope of the project at that point. So, the DS team spent many hours reducing the size of that list to something a little more manageable and realistic. . The main method of doing so was to group the items based primarily on visual similarity. We knew it was also out of the scope of our time with the project to train a model that could tell the difference between #2 plastic bottles and #3 plastic bottles, or motor oil bottles and brake fluid bottles. . . Given enough time and resources, who knows? Maybe we could train a model that accurately recognizes 300+ items and distinguishes between similar-looking items. But we had to keep our scope realistic to be sure that we actually finished something useful in the time we had. . We also considered the items that 1) users would be throwing away on a somewhat regular basis, and 2) users would usually either be unsure of how to dispose of properly or would dispose of properly. More accuracy on the important and/or common items would be more valuable to users. Some items were not grouped. . By the end of this process, we managed to cluster and prune the original list of about 300 items and materials down to 73. . Image Data Pipelines . We figured that in order to get through gathering and labeling 70,000+ images with only four people, within our timeframe, and without any budget whatsoever, we had to get creative and automate as much of the process as possible. . As explained below, the image gathering step required significant manual labor. However, we had a plan in place for automating most of the rest of the process. Below is a general outline of the image processing and labeling pipeline we built. . Rename the images to their md5sum hash to avoid duplicates and ensure unique filenames | Resize the images to save storage (and processing power later on) | Discern between transparent background and non-transparent background | If image is non-transparent, remove the background | Automatically draw bounding boxes around the object (the foreground) | If image is transparent, add a background | The beauty of automation is once the initial infrastructure is built and working, the volume can be scaled up indefinitely. The auto-labeling functionality was not perfect by any means. But it still felt great watching this pipeline rip through large batches of images. . Gather . The first part of the overall pipeline was gathering the images—around 1,000 for each of the 73 classes. This was a small pipeline in its own right, which unfortunately involved a fair bit of manual work. . Timothy built the piece of the image gathering pipeline that allowed us to at least automate some of it—the downloading part. Bing ended up being the most fruitful source of images for us. Before starting we expected to use Google Images, but pivoted when it turned out that Bing&#39;s API was much more friendly to scraping and/or programmatically downloading. . Timothy&#39;s blog post about the Trash Panda project can be found here: Games by Tim - Trash Panda. . We used his script, which in turn used a Python package called Bulk-Bing-Image-downloader, to gather the majority of images. (I say majority because we also used some images from Google&#39;s Open Images Dataset and COCO.) . The gathering process was a mixture of downloading and sifting. As we were pulling images straight from the internet, irrelevant images inevitably found their way into the search queries somehow. For each class, we went through a iterative (and somewhat tedious) loop until we had around 1,000 images of that class. The steps were simple, yet time-consuming: . Gather a batch of images (usually several hundred at a time) from Bing using a script written by Timothy | Skim through them, removing irrelevant and/or useless images | The latter was what made this pipeline difficult to fully automate, as we couldn&#39;t think of any good way of automatically vetting the images. (Though I did write a script to help me sift through images as quickly as possible, which will be the topic of a future blog post.) We had to be sure the images we were downloading in bulk actually depicted the class of item in question. . As they say, &quot;garbage in, garbage out.&quot; . And in case you weren&#39;t aware, the internet is full of garbage. . Annotate . To train an object detection model, each image in the training dataset must be annotated with rectangular bounding boxes (or, more accurately, the coordinates that define the bounding box) surrounding each of the objects belonging to a class that we want the model to recognize. These are used as the label, or target, for the model—i.e. what the model will be trying to predict. . Trevor came up with an idea to automate the labeling part of the process—arguably the most time-intensive part. Basically, the idea was to use images that feature items over transparent backgrounds. All of the major search engines allow an advanced search for transparent images. If the item is the only object in the image, it is relatively simple and straightforward to write a script that draws a bounding box around it. . If you&#39;d like some more detail on this, Trevor wrote a blog post about it. . automated bbox . Automated Background Removal . There was one big issue with this auto-labeling process. Finding a thousand unique images of a single class of object is already something of a task. And depending on the object, finding that many without backgrounds is virtually impossible. . For the images that had backgrounds, we would either have to manually label them, or find a way to automate the process and build it into the pipeline. Because the script to label images without backgrounds was already written and working, we decided to find a way of automatically removing the background from images. . This is the part of the pipeline that I built. . I&#39;ll give a brief overview here of how I built a system for automatically removing backgrounds from images. If you&#39;re curious about the details, I wrote a separate blog post on the topic: . Automated Image Background Removal with Python I tested out a few different methods of image background removal, with varying degrees of success. The highest quality results came from creating a batch task in Adobe Photoshop, as whatever algorithm powers its &quot;Select Foreground&quot; functionality is very consistent and accurate. However, we could not use this method because it could not be added to the pipeline—I was the only one with a license, meaning the speed of my computer and internet could cause a major bottleneck. . Another method I tested out was the OpenCV implementation of an algorithm called GrabCut. Long story short, I wasn&#39;t able to get the quality we needed from it, as can be seen below. . . The main issue is that the algorithm is &quot;interactive&quot;. That is, it uses the coordinates of a rectangle surrounding the object in the image for which the foreground should be extracted. For best results, the coordinates are generated manually for each image. The tighter that rectangle surrounds the foreground, the better the outline will be. . The above image is my attempt at simply using the entire image as the rectangle. As can be seen below, the result was much better when I tightened the rectangle around the can. I tried for many hours to find an automatable solution to this, but could not get results that were quite good enough. I decided to move onto other strategies. . . Ultimately, I ended up building a short image processing pipeline that utilized a pre-trained image segmentation model (similar to object detection) to find the object(s) in the image. I initially built the pipeline with a library called Detectron2, based on PyTorch. However, after running into some issues with it, I decided to reimplement the pipeline using Mask R-CNN, based on TensorFlow. . . Part of the output of the image segmentation model is a series of coordinates that describe an outline of the object(s) in the image. I used that as a binary mask to define the area of the image that should be kept, making the rest of it transparent. . . Unfortunately, I did not have much time to spend on improving the performance of the image segmentation model, and as a result there was still a fair amount of manual labeling to be done after the pipeline. I could (and should) have trained the image segmentation model using a small subset of images from each class. This would&#39;ve made the output mask much more accurate and reduced the time spent fixing the labels afterwards. . As it was, using only the pretrained weights, there were some object classes that it performed very well on, while for others it did not. . Running the Pipeline . As with building the pipeline, we split up the 73 classes evenly amongst the four of us (around 18 classes each) and got to work gathering and labeling the images. . If we&#39;d had a couple more weeks to spend improving the pipeline, this process likely would not have taken so long or been so tedious. As it was, we spent the better part of 4 weeks gathering and labeling the dataset. . I believe the pipeline did save us enough time to make it worth the extra time it took to build. Plus, I got to learn and apply some cool new tech! . . The Model . Architecture . The neural network architecture we used to train the main object detection system used in the app is called YOLOv3: You Only Look Once, version 3. . . YOLOv3 is a state-of-the-art single-shot object detection system originally written in C. One of the main reasons we used this one in particular was its speed—a benefit of single-shot algorithms. With the YOLO algorithms, object detection can be run in real time. For example, it could be run on a live security camera feed, detecting when someone enters a property. . Training . The vast majority of the work we put into this model was building a high-quality dataset. We used transfer learning to give the model an initial training kickstart. We were able to benefit from previous training done by the algorithm&#39;s developers by loading the weights from convolutional layers that were trained on ImageNet. . Our model was trained on a GPU-enabled AWS Sagemaker instance. After about 60 hours, our model reached a total average precision of 54.71%. . As expected, the model performs much better on certain classes of objects. The more easily recognizable classes (tires, printers, disks, digital cameras, plastic bottles) had average precisions in the 80-90% range. . On the other hand, the lower precision object classes were usually those that took on a wider variety of shapes, textures, and colors. For example, musical instruments, food waste, office supplies. It makes sense that, given a similar amount of training data, classes like this would be more difficult to distinguish. . Here is the breakdown for the 13,000 weights - mAP (mean average precision): . #collapse detections_count = 82369, unique_truth_count = 10934 class_id = 0, name = aerosol_cans, ap = 54.87% (TP = 54, FP = 45) class_id = 1, name = aluminium_foil, ap = 42.11% (TP = 32, FP = 22) class_id = 2, name = ammunition, ap = 55.38% (TP = 61, FP = 49) class_id = 3, name = auto_parts, ap = 41.70% (TP = 31, FP = 21) class_id = 4, name = batteries, ap = 62.19% (TP = 92, FP = 44) class_id = 5, name = bicycles, ap = 79.86% (TP = 86, FP = 22) class_id = 6, name = cables, ap = 64.81% (TP = 76, FP = 40) class_id = 7, name = cardboard, ap = 52.99% (TP = 50, FP = 41) class_id = 8, name = cartridge, ap = 70.16% (TP = 68, FP = 25) class_id = 9, name = cassette, ap = 53.45% (TP = 13, FP = 10) class_id = 10, name = cd_cases, ap = 80.11% (TP = 30, FP = 3) class_id = 11, name = cigarettes, ap = 29.43% (TP = 38, FP = 56) class_id = 12, name = cooking_oil, ap = 69.23% (TP = 61, FP = 25) class_id = 13, name = cookware, ap = 70.76% (TP = 81, FP = 83) class_id = 14, name = corks, ap = 57.99% (TP = 55, FP = 32) class_id = 15, name = crayons, ap = 52.43% (TP = 44, FP = 25) class_id = 16, name = desktop_computers, ap = 75.17% (TP = 34, FP = 12) class_id = 17, name = digital_cameras, ap = 93.40% (TP = 120, FP = 15) class_id = 18, name = disks, ap = 90.14% (TP = 90, FP = 25) class_id = 19, name = doors, ap = 0.00% (TP = 0, FP = 0) class_id = 20, name = electronic_waste, ap = 73.97% (TP = 50, FP = 16) class_id = 21, name = eyeglasses, ap = 21.75% (TP = 24, FP = 37) class_id = 22, name = fabrics, ap = 52.17% (TP = 69, FP = 52) class_id = 23, name = fire_extinguishers, ap = 30.32% (TP = 22, FP = 30) class_id = 24, name = floppy_disks, ap = 78.26% (TP = 83, FP = 53) class_id = 25, name = food_waste, ap = 19.16% (TP = 28, FP = 22) class_id = 26, name = furniture, ap = 3.45% (TP = 0, FP = 0) class_id = 27, name = game_consoles, ap = 58.90% (TP = 45, FP = 19) class_id = 28, name = gift_bags, ap = 65.97% (TP = 72, FP = 48) class_id = 29, name = glass, ap = 72.21% (TP = 149, FP = 49) class_id = 30, name = glass_container, ap = 0.00% (TP = 0, FP = 0) class_id = 31, name = green_waste, ap = 57.38% (TP = 61, FP = 55) class_id = 32, name = hardware, ap = 24.78% (TP = 28, FP = 62) class_id = 33, name = hazardous_fluid, ap = 79.06% (TP = 85, FP = 10) class_id = 34, name = heaters, ap = 71.18% (TP = 54, FP = 50) class_id = 35, name = home_electronics, ap = 32.91% (TP = 83, FP = 92) class_id = 36, name = laptop_computers, ap = 41.66% (TP = 95, FP = 55) class_id = 37, name = large_appliance, ap = 5.91% (TP = 7, FP = 38) class_id = 38, name = lightbulb, ap = 28.36% (TP = 61, FP = 28) class_id = 39, name = medication_containers, ap = 59.85% (TP = 88, FP = 46) class_id = 40, name = medications, ap = 55.37% (TP = 68, FP = 39) class_id = 41, name = metal_cans, ap = 52.21% (TP = 82, FP = 24) class_id = 42, name = mixed_paper, ap = 37.32% (TP = 64, FP = 43) class_id = 43, name = mobile_device, ap = 63.24% (TP = 151, FP = 97) class_id = 44, name = monitors, ap = 39.15% (TP = 75, FP = 77) class_id = 45, name = musical_instruments, ap = 20.67% (TP = 44, FP = 42) class_id = 46, name = nail_polish, ap = 84.06% (TP = 105, FP = 30) class_id = 47, name = office_supplies, ap = 7.01% (TP = 30, FP = 52) class_id = 48, name = paint, ap = 47.56% (TP = 54, FP = 25) class_id = 49, name = paper_cups, ap = 76.20% (TP = 85, FP = 32) class_id = 50, name = pet_waste, ap = 31.60% (TP = 5, FP = 1) class_id = 51, name = pizza_boxes, ap = 61.07% (TP = 69, FP = 38) class_id = 52, name = plastic_bags, ap = 31.45% (TP = 39, FP = 36) class_id = 53, name = plastic_bottles, ap = 77.49% (TP = 303, FP = 92) class_id = 54, name = plastic_caps, ap = 22.99% (TP = 5, FP = 1) class_id = 55, name = plastic_cards, ap = 57.51% (TP = 45, FP = 14) class_id = 56, name = plastic_clamshells, ap = 74.12% (TP = 36, FP = 16) class_id = 57, name = plastic_containers, ap = 71.93% (TP = 104, FP = 47) class_id = 58, name = power_tools, ap = 75.21% (TP = 77, FP = 58) class_id = 59, name = printers, ap = 88.06% (TP = 106, FP = 62) class_id = 60, name = propane_tanks, ap = 62.89% (TP = 36, FP = 15) class_id = 61, name = scrap_metal, ap = 0.00% (TP = 0, FP = 0) class_id = 62, name = shoes, ap = 83.53% (TP = 64, FP = 12) class_id = 63, name = small_appliances, ap = 80.61% (TP = 93, FP = 34) class_id = 64, name = smoke_detectors, ap = 79.73% (TP = 65, FP = 10) class_id = 65, name = sporting_goods, ap = 3.34% (TP = 0, FP = 0) class_id = 66, name = tires, ap = 83.31% (TP = 77, FP = 11) class_id = 67, name = tools, ap = 69.69% (TP = 113, FP = 37) class_id = 68, name = toothbrushes, ap = 2.92% (TP = 1, FP = 1) class_id = 69, name = toothpaste_tubes, ap = 59.78% (TP = 35, FP = 9) class_id = 70, name = toy, ap = 49.70% (TP = 47, FP = 24) class_id = 71, name = vehicles, ap = 4.51% (TP = 1, FP = 4) class_id = 72, name = water_filters, ap = 47.91% (TP = 36, FP = 21) class_id = 73, name = wood, ap = 76.15% (TP = 105, FP = 51) class_id = 74, name = wrapper, ap = 72.45% (TP = 67, FP = 18) for conf_thresh = 0.25, precision = 0.65, recall = 0.41, F1-score = 0.50 for conf_thresh = 0.25, TP = 4507, FP = 2430, FN = 6427, average IoU = 51.08 % IoU threshold = 50 %, used Area-Under-Curve for each unique Recall mean average precision (mAP@0.50) = 0.523220, or 52.32 % Total Detection Time: 564 Seconds Set -points flag: `-points 101` for MS COCO `-points 11` for PascalVOC 2007 (uncomment `difficult` in voc.data) `-points 0` (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset . . Deployment . In order to utilize our hard-earned trained weights for inference in the app, I built an object detection API with Flask and used Docker to deploy it to AWS Elastic Beanstalk. . The trained weights were loaded into a live network using OpenCV, which can then be used for inference (prediction; detecting objects). Once a user takes a photo in the app, it is encoded and sent to the detection API. The API decodes the image and passes it into the live network running with the trained weights. The network runs inference on the image and sends back the class of item with the highest predicted probability. . Again, I go into much greater detail on the API in a separate blog post (link to come, once it&#39;s published). . . Final Thoughts . Potential Improvements . Although we had eight weeks for this project, we were starting with a completely green field. Therefore, at least the first two weeks were dedicated to discussions and planning. . The result of scope management is features and ideas left unimplemented. At the beginning, the entire team brainstormed a lot about potential ideas for the app. And throughout the project, the machine learning team was always thinking and discussing potential improvements. Here is a brief summary of some of them. . The first and most obvious one is to simply expand the model by gathering and labeling an even wider variety and even greater numbers of images. By breaking out the grouped items into more specific classes and gathering more images, the model could be trained to distinguish between some of the visually similar (though materially different) objects. . Furthermore, there are some low-hanging fruit that would likely make the model more robust. One of those that we did not get to explore due to lack of time is image augmentation. In a nutshell, image augmentation is a method of artificially growing an image dataset by &quot;creating&quot; new images from the existing ones by transforming or otherwise manipulating them. . These augmentations could include changing the color, crop, skew, rotation, and even combining multiple images. The labels are also transformed to match the location and size of the transformed objects in each image. Not only does this process help create more training data, but it can also help the model extract more of the underlying features of the objects, allowing it to be more robust to a greater variety of lighting conditions and camera angles. . Another way to increase the size and variety of the dataset would be to add a feature that gathers the images (and labels) taken by the user. After every prediction that is served, the user could be presented with a box containing two buttons, one indicating that the prediction was correct, the other, incorrect. The correct ones could go straight to the server to wait until the next model training round. And if the user indicates that the predicted class was incorrect, they could be presented with an interface to draw a box around the object and choose the correct class. Then that image and label would be sent to the server to be used in training. . The last idea was one that the entire team discussed at one point, though is a feature that would likely only be possible with some kind of financial backing. The idea is to implement a system that recognizes when many of a similar type of object are detected—those need to be recycled at the same facility, for example—and can pay drivers to pick all of those items up and take them to the proper facilities. In other words, be the &quot;UBER for recycling&quot;. . Team Links . I said it before and I&#39;ll say it again: the team I worked with on this project was top-notch. Here they are in all of their glory, with all of their portfolios/blogs/GitHubs (those that have them) in case you want to check out their other work. . Machine learning . Tobias Reaper (though you&#39;re already here!) | Trevor Clack | Vera Mendes | Timothy Hsu | . Web development . Mark Halls | Mark Artishuk | Colin Bazzano | Carlo Lucido | . User Experience . Kendra McKernan | Lynn Baxter | . [[ImageBlock]] [Image of all team members] . Other Links . I linked to this video above but figured I&#39;d include it here as well. It&#39;s the video that Trevor created to pitch the idea (originally called &#39;Recycle This?&#39;) to the Lambda Labs coordinators. Also included is another video he created to demo the final product. . Recycle This? Project Proposal | Trash Panda Demo | . I was one of the presenters of our app for the final demos to both our cohort and to the entire school. Links to those videos are below. . Lambda Labs Shark Tank demo presentation | Lambda Labs cohort (LABS20) presentation | . Here is a video (thanks to Trevor) showing the progress of the interface at various stages throughout the project. . Trash Panda UI Progress | . As always, thanks for reading, and I&#39;ll see you in the next one! .",
            "url": "https://tobias-fyi.github.io/fyi-pages/2020/03/08/trash-panda.html",
            "relUrl": "/2020/03/08/trash-panda.html",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://tobias-fyi.github.io/fyi-pages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tobias-fyi.github.io/fyi-pages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tobias-fyi.github.io/fyi-pages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tobias-fyi.github.io/fyi-pages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}